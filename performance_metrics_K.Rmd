---
title: "compute_metrics"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(purrr)
library(ggpubr)
```
# Comparing model and observed time series output

  
```{r simple}
sager = read.table("sager.txt", header=T)
head(sager)

# add date
sager = sager %>% mutate(date = paste(day,month,year, sep="/"))
sager$date = as.Date(sager$date,"%d/%m/%Y")

# plot
sagerl = sager %>% pivot_longer(cols=c("model","obs"), names_to="source",
                                  values_to="flow")

# basic plot
ggplot(sagerl, aes(date, flow, col=source, linetype=source))+geom_line()
```


```{r simple2}
# change axis to get a closer look at performance at low values
# when you have high dynamic range (lots of large and small values), taking log can help
# with visualization
ggplot(sagerl, aes(date, flow, col=source, linetype=source))+geom_line()+scale_y_continuous(trans="log")+labs(y="streamflow mm/day")
```


```{r simple3}
# look at it another way
ggplot(sager, aes(obs, model))+geom_point()+geom_abline(intercept=0, slope=1, col="red")


```

# Measure Performance using different metrics

```{r}


source("combined_metrics_K.R")

combined_function(m=sager$model, o=sager$obs)

```

# Scale and subsetting
  
```{r}

# look at rainy season, Nov to March
sager_filtered <- sager %>% 
  filter(month %in% c(11, 12, 1, 2, 3)) %>%
  group_by(month, year) %>%
  summarize(model = sum(model), obs = sum(obs))

# turn your evaluation metric into a function
source("high_flow_K.R")
high_flow_metrics_K
high_flow_metrics_K(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy)

# use different high flow months
#change from august to Nov - March
high_flow_metrics_K(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy, high_flow_months = c(11:3))


```
  

# try another metric


# Create a combined metric

Sometime you want to summarize everything in one number

Especially if you want to rank different models
or create indices like Sobol Sensitivity Indices

```{r}

perf = high_flow_metrics_K(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy, high_flow_months = c(11:3))

perf = as.data.frame((perf))
# remember you want error to be low but correlation to be high 
# so we need to transform in some way

# normalize by max error = if error is greater than this we don't care
# many ideas -  maybe 50% of mean daily summer observed low flow
tmp = sager %>% subset(month %in% c(11:3)) 
errmin = mean(tmp$obs)*0.5

perf = perf %>% mutate(annual_max_err_trans = min(0,(1-abs(annual_max_err/errmin) )))
      
# for monthly we can do a simpler thing to find maximum allowable errror   
tmp = sager %>% subset(month %in% c(11:3)) %>% group_by(wy, month) %>% summarize(obs=sum(obs))

errmax = mean(tmp$obs)*0.5
 
perf = perf %>% mutate(high_month_err_trans = min(0,(1-abs(high_month_err/errmin) )))

# now we have 4 measures that we can combine together

perf = perf %>% mutate(combined = (annual_max_cor + annual_max_err_trans + high_month_err_trans + high_month_cor)/4)
perf
# or weight differently - we know that minimum flows are hard to get to weight those differently

perf = perf %>% mutate(combined2 = 0.1*annual_max_cor + 0.1*annual_max_err_trans + 0.4*high_month_err_trans+ 0.4*high_month_cor)

perf

# easier to put all this in a function


```


# Calibration Not sharted yet

File Name
* sagerm.txt



```{r multipel}
# multiple results - lets say we've run the model for multiple years, each column
# is streamflow for a different parameter set
msage = read.table("sagerm.txt", header=T)

# lets say we know the start date from our earlier output
msage$date = sager$date
head(msage)
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# and we still have observed data from above


# how can we plot all results - lets plot water year 1970 otherwise its hard to see
msagel = msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")

p1=ggplot(subset(msagel, wy == 1970), aes(as.Date(date), flow, col=run))+geom_line()+theme(legend.position = "none")
p1
# lets add observed streamflow
p1+geom_line(data=subset(sager, wy == 1970), aes(as.Date(date), obs), size=2, col="black", linetype=2)+labs(y="Streamflow", x="Date")


# compute performance measures for all output
res = msage %>% select(!c("date","month","year","day","wy")) %>%
      map_dbl(nse, o=sager$obs )

head(res)
```


```{r multipel}
# another example using our low flow statistics
# use apply to compute for all the data
# using the updated low flow metrics routing that also computed combined metrics

source("compute_lowflowmetrics_all.R")
res = msage %>% select(-date, -month, -day, -year, -wy ) %>% map_df(compute_lowflowmetrics_all, o=sager$obs, month=msage$month, day=msage$day, year=msage$year, wy=msage$wy)


# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)


# graph range of performance measures
resl = res %>% pivot_longer(cols=everything(), names_to="metric", values_to="value")
ggplot(resl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")

# try this
# assign an identifier to each row, use the same identify for columns of original streamflow data
# we can then use that to pick data
res$run = seq(from=1,to=nrow(res))
head(msage)
colnames(msage)=c(res$run, "date","month","year","day","wy")

# best one
best = res[which.max(res$combined),]
msagel  =  msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")
ggplot(subset(msagel, run == best$run), aes(date, flow)) + geom_line()



```